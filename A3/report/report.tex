\documentclass[a4paper]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings} % iPackage for code
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}

\title{CSC411 Fall 2017\\Assignment 3 Report}
\author{Tianbao Li}
\date{2017/12/04}

\begin{document}

\maketitle

\section{20 Newsgroups prediction}

\subsection{Data summary}

Here is the detailed information of the dataset:

\begin{itemize}
    \item data set: \href{http://scikit-learn.org/stable/datasets/twenty_newsgroups.html}{The 20 newsgroups text}
    \item data amount: 11314
    \item feature amount: 101631
    \item data representation: tf-idf
    \item baseline: Bernoulli Naive-Bayes classifier
\end{itemize}

\subsection{Model training}

To train model to outperform the baseline, here we useopen-souece code of \href{http://scikit-learn.org/stable/}{scikit-learn}. Here, I picked several different algorithms and trained their hyper-parameters with k-fold validation. The detailed better hyper-parameters, train losses (0-1 loss), test losses (0-1 loss) are shown in Table~\ref{tab: Model_comparison}. From the result of the validation losses, the best three models are Neural Networks, Multinomial Naive Bayes, Logist Regression, and has the same best results in test dataset.

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    Model & Hyper-pramater & Train loss & test loss\\
    \hline
    BernoulliNB &  & 0.598727240587 & 0.457912904939\\
    \hline
    MultinomialNB & $\alpha=0.01$ & 0.958900477285 & 0.700212426978\\
    \hline
    Logist Regression & $C=500$ & 0.974721583878 & 0.683483802443\\
    \hline
    SGD & $\alpha=0.0001$ & 0.962877850451 & 0.671136484334\\
    \hline
    SVM & $C=1$ & 0.895704436981 & 0.67750929368\\
    \hline
    KNN & $K=1$ & 0.973749337104 & 0.113382899628\\
    \hline
    Decision Tree & $K=601$ & 0.974721583878 & 0.4026818906\\
    \hline
    Neural Networks & $\alpha=0.0001$ & 0.947587060279 & 0.712294211365\\
    \hline
    \end{tabular}
\caption{Model comparison}
\label{tab: Model_comparison}
\end{table}

\subsection{Hyper-parameters training}

To train models by choosing better-fitting parameters, I splited training data by \href{http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html}{KFold} and run cross valiadation by \href{http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html}{cross\_val\_score}.

Here, taking multinomial Naive Bayes as an example. Hyper-parameter $\alpha$ is used for smoothing. I picked several possible $\alpha$ value and compared by validation loss, then chose a better $\alpha$.

\begin{lstlisting}[language = Python]
splits = 5
kf = KFold(splits, shuffle = True, random_state = 0)
As = [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 0.1, 0.5, 1.0, 2, 5, 10]
scores = []
for a in As:
    model = MultinomialNB(alpha = a)
    score = cross_val_score(model, tfidf_train, train_labels, cv = kf)
    scores.append(np.mean(score))
opt_A_index = int(np.argmax(scores))
opt_A = As[opt_A_index]
\end{lstlisting}

\subsection{Model selection}

For the three well-working models, they have their advantages for solving this problem.

\begin{itemize}
    \item \textbf{Neural Networks} can work well for complex models, especially when it comes deeper.
    \item \textbf{Multinomial Naive Bayes} implements the naive Bayes algorithm for multinomially distributed data, text classification usually holds such structure.
    \item \textbf{Logistic Regression} is famous for linear classification, which the newsgroup data has similar distribution.
\end{itemize}

\subsection{Class confusion}

For the best classifier, Neural Networks, the confusion matrix among 20 groups is shown in Table~\ref{tab: Confusion_matrix}. The two classed that Neural Networks classifier confuses about are class 16 and 18.

\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{|cccccccccccccccccccc|}
    \hline
    163 & 6 & 5 & 1 & 1 & 0 & 0 & 4 & 4 & 7 & 5 & 3 & 2 & 11 & 7 & 20 & 7 & 28 & 20 & 38\\
    \hline
    1 & 287 & 26 & 15 & 6 & 49 & 2 & 1 & 1 & 2 & 2 & 8 & 11 & 8 & 11 & 3 & 1 & 1 & 1 & 4\\
    \hline
    3 & 19 & 241 & 37 & 10 & 31 & 2 & 2 & 1 & 0 & 0 & 5 & 15 & 1 & 2 & 1 & 2 & 0 & 0 & 2\\
    \hline
    1 & 9 & 39 & 256 & 22 & 7 & 18 & 1 & 1 & 0 & 0 & 2 & 26 & 1 & 0 & 0 & 1 & 2 & 0 & 2\\
    \hline
    1 & 9 & 12 & 32 & 284 & 5 & 14 & 1 & 1 & 0 & 0 & 2 & 11 & 2 & 1 & 0 & 2 & 0 & 0 & 0\\
    \hline
    0 & 16 & 12 & 4 & 3 & 278 & 0 & 0 & 0 & 0 & 0 & 1 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
    \hline
    0 & 4 & 3 & 10 & 8 & 4 & 305 & 10 & 5 & 3 & 0 & 1 & 10 & 3 & 3 & 0 & 1 & 0 & 0 & 1\\
    \hline
    15 & 7 & 17 & 9 & 21 & 5 & 20 & 322 & 32 & 15 & 12 & 19 & 21 & 22 & 23 & 14 & 19 & 8 & 13 & 10\\
    \hline
    3 & 5 & 4 & 0 & 4 & 1 & 6 & 16 & 312 & 3 & 1 & 2 & 10 & 7 & 5 & 1 & 4 & 7 & 1 & 2\\
    \hline
    4 & 2 & 1 & 0 & 1 & 2 & 3 & 3 & 3 & 334 & 11 & 8 & 2 & 0 & 1 & 3 & 2 & 3 & 0 & 1\\
    \hline
    2 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 14 & 354 & 0 & 0 & 4 & 1 & 0 & 0 & 1 & 2 & 0\\
    \hline
    2 & 6 & 6 & 2 & 2 & 4 & 1 & 2 & 1 & 1 & 0 & 289 & 13 & 0 & 0 & 0 & 7 & 3 & 1 & 1\\
    \hline
    4 & 9 & 2 & 24 & 21 & 5 & 8 & 15 & 18 & 3 & 1 & 14 & 245 & 10 & 9 & 1 & 2 & 1 & 3 & 1\\
    \hline
    2 & 0 & 2 & 0 & 0 & 0 & 0 & 1 & 3 & 3 & 1 & 2 & 8 & 300 & 5 & 2 & 2 & 2 & 5 & 6\\
    \hline
    9 & 7 & 11 & 1 & 1 & 3 & 3 & 3 & 6 & 1 & 1 & 3 & 9 & 3 & 302 & 4 & 7 & 0 & 8 & 6\\
    \hline
    53 & 1 & 3 & 0 & 0 & 1 & 1 & 1 & 2 & 4 & 4 & 3 & 2 & 7 & 6 & 321 & 9 & 13 & 5 & 68\\
    \hline
    9 & 1 & 2 & 0 & 1 & 0 & 3 & 2 & 3 & 2 & 2 & 19 & 1 & 5 & 4 & 0 & 258 & 6 & 89 & 20\\
    \hline
    9 & 1 & 2 & 0 & 0 & 0 & 1 & 2 & 0 & 1 & 2 & 3 & 3 & 3 & 1 & 0 & 8 & 287 & 6 & 6\\
    \hline
    7 & 0 & 4 & 0 & 0 & 0 & 2 & 5 & 4 & 4 & 2 & 10 & 2 & 8 & 13 & 5 & 18 & 13 & 149 & 11\\
    \hline
    31 & 0 & 2 & 0 & 0 & 0 & 1 & 4 & 1 & 0 & 1 & 2 & 0 & 1 & 0 & 23 & 14 & 1 & 6 & 72\\
    \hline
    \end{tabular}
\caption{Confusion matrix}
\label{tab: Confusion_matrix}
\end{table}

\section{Training SVM with SGD}

\subsection{SGD with momentum}

Parameters:
\begin{itemize}
    \item function: $f(w)=0.01w^2$
    \item initianlization: $w_0=10.0$
    \item learning rate: $\alpha=1.0$
    \item momentum: $\beta_1=0.0, \beta_2=0.9$
\end{itemize}

Changes for $w_t$ is shown as Figure~\ref{fig: SGD}.

\begin{figure}[htbp]
\centering
\includegraphics[width = 8cm]{SGD}
\caption{$w_t$ for $\beta=0.0$ and $0.9$}
\label{fig: SGD}
\end{figure}

\subsection{Training SVM}

Given the formula of SVM, the gradient during training can be shown as

\begin{equation}
\frac{\partial \ell}{\partial w}=\left\{
\begin{aligned}
-C*y^{(i)}\mathbf{x}^{(i)}+w & = & y^{(i)}(w^{\mathrm{T}}\mathbf{x}^{(i)}+b)<1 \\
0 & = & y^{(i)}(w^{\mathrm{T}}\mathbf{x}^{(i)}+b)\ge1
\end{aligned}
\right.
\end{equation}

\subsection{Apply on 4-vs-9 digits on MNIST}

Here, I trained SVM on MNIST. The trained models for $\beta=0$ is reported as follows:

\begin{itemize}
    \item $\beta$: 0.0
    \item optimal hyper-parameter: $C=1$
    \item training loss: 0.748119588312
    \item test loss: 0.720131532536
    \item training accuracy: 0.927437641723
    \item test accuracy: 0.92636924193
\end{itemize}

The trained models for $\beta=0.0$ is reported as follows:

\begin{itemize}
    \item $\beta$: 0.1
    \item optimal hyper-parameter: $C=1$
    \item training loss: 0.483890097883
    \item test loss: 0.500220862748
    \item training accuracy: 0.940770975057
    \item test accuracy: 0.937250634748
\end{itemize}

The $w$ figures for $\beta=0.0$ and $0.1$ are shown in Figure ~\ref{fig: SVM}.

\begin{figure}[htbp]
\centering
\includegraphics[width = 8cm]{SVM}
\caption{$w$ figure for $\beta=0.0$ (left) and $0.1$ (right)}
\label{fig: SVM}
\end{figure}

\section{Kernels}

\subsection{Positive semidefinite and quadratic form}

Proof that a symmetric matrix $K\in\mathbbm{R}^{d\times d}$ is positive semidefinite iff for all vectors $\mathbf{x}\in\mathbbm{R}^d$ we have $\mathbf{x}^\mathrm{T}K\mathbf{x}\ge0$.

\begin{proof}
    Necessity:
    \begin{align*}
        \because&\forall \mathbf{x}\in\mathbbm{R}^d, \mathbf{x}^\mathrm{T}K\mathbf{x}\ge0\\
        \therefore&\forall v \text{ as eigenvector of } K, v^\mathrm{T}Kv=v^\mathrm{T}\lambda v\ge0\\
        \therefore&\lambda\ge0\text{ ($\lambda$ corresponds to $v$)}\\
        \therefore&\forall\lambda\ge0\text{ as eigenvalue of } K\\
        \therefore&K\text{ has no negative eigenvalue}\\
        \because&K\text{ is symmetric}\\
        \therefore&K\text{ is positive semidefinite}
    \end{align*}
    \quad\quad\quad Sufficiency:
    \begin{align*}
        \because&K\in\mathbbm{R}^{d\times d}\text{ is symmetric}\\
        \therefore&K\text{ has orthogonal eigenvectors for different eigenvalues}\\
        &K\text{ can be diagonalized}\\
        \therefore&K\text{ can be represented as }K=Q\Lambda Q^{-1}=Q\Lambda Q^\mathrm{T}\\
        &Q=[v_1,v_2,\dots,v_d]^\mathrm{T}:\text{ eigenvector matrix}\\
        &\Lambda:\text{ diagonal matrix}\\
        \because&v_1,v_2,\dots,v_d\text{ are diagonal}\\
        \therefore&\forall\mathbf{x}\in R^d=(c_1v_1+c_2c_2+\dots+c_dv_d)\\
        \therefore&\mathbf{x}^\mathrm{T}A\mathbf{x}=\mathbf{x}^\mathrm{T}Q\Lambda Q^\mathrm{T}\mathbf{x}\\
        &\quad\quad=(c_1v_1+c_2c_2+\dots+c_dv_d)\bigl(\begin{smallmatrix} v_1\\v_2\\\vdots\\v_d\end{smallmatrix}\bigr)\bigl(\begin{smallmatrix} \lambda_1 &  &  &\\\  & \lambda_2 &  &\\  &  & \dots &\\  &  &  & \lambda_d\\\end{smallmatrix}\bigr)(v_1,v_2,\dots,v_d)(c_1v_1+c_2c_2+\dots+c_dv_d)^\mathrm{T}\\
        &\quad\quad=(c_1||v_1||^2+c_2||v_2||^2+\dots+(c_d||v_d||^2))\bigl(\begin{smallmatrix} \lambda_1 &  &  &\\\  & \lambda_2 &  &\\  &  & \dots &\\  &  &  & \lambda_d\\\end{smallmatrix}\bigr)(c_1||v_1||^2+c_2||v_2||^2+\dots+(c_d||v_d||^2))^\mathrm{T}\\
        &\quad\quad=\lambda_1c_1^2||v_1||^4+\lambda_2c_2^2||v_2||^4+\dots+\lambda_dc_d^2||v_d||^4\ge0\\
        \therefore&\mathbf{x}^\mathrm{T}K\mathbf{x}\ge0
    \end{align*}
\end{proof}

\subsection{Kernel properties}

\subsubsection{$k(\mathbf{x},\mathbf{y})=\alpha$}

\begin{proof}
    \begin{align*}
        k(\mathbf{x},\mathbf{y})&=\alpha\\
        &=\sqrt{\alpha}\sqrt{\alpha}\\
        &=<\phi(\mathbf{x}),\phi(\mathbf{y})>\\
        \phi(x)&=\sqrt{\alpha}\\
        \therefore&K\text{ is kernel}
    \end{align*}
\end{proof}

\subsubsection{$k(\mathbf{x},\mathbf{y})=f(\mathbf{x})\cdot f(\mathbf{y})$}

\begin{proof}
    \begin{align*}
        k(\mathbf{x},\mathbf{y})&=f(\mathbf{x})\cdot f(\mathbf{y})\\
        &=<f(\mathbf{x}),f(\mathbf{y})>\\
        &=<\phi(\mathbf{x}),\phi(\mathbf{y})>\\
        \phi(x)&=f(x)\\
        \therefore&K\text{ is kernel}
    \end{align*}
\end{proof}

\subsubsection{$k(\mathbf{x},\mathbf{y})=a\cdot k_1(\mathbf{x},\mathbf{y})+b\cdot k_2(\mathbf{x},\mathbf{y})$}

\begin{proof}
    \begin{align*}
        &K_{1 i,j}=K_1(\mathbf{x}^{(i)},\mathbf{y}^{(j)})\\
        &K_{2 i,j}=K_2(\mathbf{x}^{(i)},\mathbf{y}^{(j)})\\
        \therefore&K_{i,j}=a\cdot K_1(\mathbf{x}^{(i)},\mathbf{y}^{(j)})+b\cdot K_2(\mathbf{x}^{(i)},\mathbf{y}^{(j)})\\
        &\quad\quad=a\cdot K_{1 i,j}+b\cdot K_{2 i,j}\\
        &v^\mathrm{T}K_1v=c_1>0\quad K_1v=(v^\mathrm{T})^{-1}c_1\\
        &v^\mathrm{T}K_2v=c_2>0\quad K_2v=(v^\mathrm{T})^{-1}c_2\\
        \therefore& aK_1v+bK_2v=(v^\mathrm{T})^{-1}c_1a+(v^\mathrm{T})^{-1}c_2b\\
        &(aK_1+bK_2)v=(v^\mathrm{T})^{-1}c_1a+(v^\mathrm{T})^{-1}c_2b\\
        &v^\mathrm{T}(aK_1+bK_2)v=c_1a+c_2b\\
        &v^\mathrm{T}Kv=c_1a+c_2b\ge0\\
        \therefore&K\text{ is kernel}
    \end{align*}
\end{proof}

\subsubsection{$k(\mathbf{x},\mathbf{y})=\frac{k_1(\mathbf{x},\mathbf{y})}{\sqrt{k_1(\mathbf{x},\mathbf{x})k_1(\mathbf{y},\mathbf{y})}}$}

\begin{proof}
    \begin{align*}
        k(\mathbf{x},\mathbf{y})&=\frac{k_1(\mathbf{x},\mathbf{y})}{\sqrt{k_1(\mathbf{x},\mathbf{x})k_1(\mathbf{y},\mathbf{y})}}\\
        &=\frac{<\phi(\mathbf{x}),\phi(\mathbf{y})>}{\sqrt{<\phi(\mathbf{x}),\phi(\mathbf{x})>}\sqrt{<\phi(\mathbf{y}),\phi(\mathbf{y})>}}\\
        &=\frac{<\phi(\mathbf{x}),\phi(\mathbf{y})>}{|\phi(\mathbf{x})||\phi(\mathbf{y})|}\\
        &=<\frac{\phi(\mathbf{x})}{|\phi(\mathbf{x})|},\frac{\phi(\mathbf{y})}{|\phi(\mathbf{y})|}>\\
        \therefore&\phi^,(x)=\frac{\phi(x)}{|\phi(x)|}\\
        \therefore&K\text{ is kernel}
    \end{align*}
\end{proof}

\bibliographystyle{plain}
\bibliography{bibliography.bib}
\end{document}
